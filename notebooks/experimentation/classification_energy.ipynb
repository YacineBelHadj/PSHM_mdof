{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import comet_ml\n",
    "# load the database path\n",
    "from config import settings\n",
    "from pathlib import Path\n",
    "\n",
    "settings_proc ='SETTINGS1'\n",
    "settings_simu = 'SETTINGS1'\n",
    "root= Path(settings.data.path[\"processed\"])\n",
    "database_path = (root /settings_simu/settings_simu.lower()).with_suffix('.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/yacine/Documents/PhD/Code/GitProject/PBSHM_mdof/data/processed/SETTINGS1/settings1.db')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psm.utils.data.metadata import get_metadata_processed\n",
    "metadata= get_metadata_processed(settings_proc, settings_simu)\n",
    "freq_axis = metadata['freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(anomaly_level=?) AND stage=? (0, 'train')\n",
      "(anomaly_level=?) AND stage=? (0, 'test')\n",
      "Number of training samples: 16000\n",
      "Number of validation samples: 4000\n",
      "Number of test samples: 4000\n",
      "Shape of data from train_dl: torch.Size([32, 385])\n",
      "Shape of label from train_dl: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from psm.models.prepare_data import CreateTransformer,PSDDataModule,PSDDataset\n",
    "# let's create the transformer\n",
    "transformer = CreateTransformer(database_path, freq=freq_axis, freq_min=0, freq_max=150)\n",
    "transform_psd = transformer.transform_psd\n",
    "transform_label = transformer.transform_label\n",
    "input_dim = transformer.dimension_psd()\n",
    "dm = PSDDataModule(database_path, transform_psd, transform_label, batch_size=32)\n",
    "dm.setup()\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = dm.train_dataloader()\n",
    "val_dl = dm.val_dataloader()\n",
    "test_dl = dm.test_dataloader()\n",
    "\n",
    "# Print some details\n",
    "print(f\"Number of training samples: {len(dm.train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(dm.val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(dm.test_dataset)}\")\n",
    "\n",
    "# Get a batch of data\n",
    "for batch in train_dl:\n",
    "    data, label = batch\n",
    "    print(f\"Shape of data from train_dl: {data.shape}\")\n",
    "    print(f\"Shape of label from train_dl: {label.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage=? ('anomaly',)\n",
      "stage=? ('test',)\n",
      "system_name LIKE ? ('%',)\n",
      "system_name LIKE ? ('%',)\n"
     ]
    }
   ],
   "source": [
    "from psm.models.prepare_data import PSDNotchDataset , PSDNotchDatasetOriginal\n",
    "database_notch_path = (root/settings_simu/(settings_proc.lower()+\"_vas\")).with_suffix('.db')\n",
    "\n",
    "anomaly_dl = PSDDataset(database_path, transform=transform_psd, transform_label=transform_label, stage='anomaly')\n",
    "test_dl = PSDDataset(database_path, transform=transform_psd, transform_label=transform_label, stage='test')\n",
    "\n",
    "\n",
    "psd_notch = PSDNotchDataset(database_notch_path, transform=transform_psd, transform_label=transform_label)\n",
    "psd_original = PSDNotchDatasetOriginal(database_notch_path,transform=transform_psd, transform_label=transform_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    }
   ],
   "source": [
    "from psm.models.callbacks_logger import log_metrics \n",
    "from psm.models.energy_classification import EnergyDenseSignalClassifierModule \n",
    "from psm.models.callbacks_logger import create_callbacks_loggers\n",
    "from pytorch_lightning import Trainer\n",
    "from psm.models.ad_systems import AD_energy\n",
    "from psm.eval.benchmark import Benchmark_SA, Benchmark_VAS\n",
    "from torch import nn\n",
    "\n",
    "# create logger and callbacks\n",
    "callbacks, logger = create_callbacks_loggers()\n",
    "# hyperparameters\n",
    "hyper_params = {'input_dim':input_dim,'en_coef':0.3, 'dense_layers':[512,248,248,128, 64, 32],\n",
    "                'dropout_rate':0, 'num_classes':20, 'lr':0.002,\n",
    "                'batch_norm':True, 'activation':nn.ReLU(), 'l1_reg':1e-4}\n",
    "# create model\n",
    "model = EnergyDenseSignalClassifierModule(**hyper_params)\n",
    "\n",
    "trainer = Trainer(max_epochs=100, callbacks=callbacks, logger=logger)\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = EnergyDenseSignalClassifierModule.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "trainer.test(best_model, dm)\n",
    "ad_system = AD_energy(model=best_model.model)\n",
    "ad_system.fit(train_dl)\n",
    "#ad_system.log_model(logger.experiment)\n",
    "logger.experiment.log_model(\"best_model\", checkpoint_path)\n",
    "logger.experiment.log_asset(checkpoint_path,step=trainer.global_step)\n",
    "\n",
    "\n",
    "# first benchmark\n",
    "benchmark1= Benchmark_SA(ad_system,anomaly_dl, test_dl, batch_size=10000)\n",
    "res1 = benchmark1.evaluate()\n",
    "benchmark2= Benchmark_VAS(ad_system,psd_notch,psd_original,batch_size=10000)\n",
    "res2 = benchmark2.evaluate_all_systems()\n",
    "global_metric = log_metrics(logger,res1,res2)\n",
    "\n",
    "# After training, you can also test your model\n",
    "\n",
    "logger.experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBSHM_Mdof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
